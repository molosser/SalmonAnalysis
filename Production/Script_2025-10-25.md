
🧑‍🔬 三文鱼专业分析架构

1️⃣ 目标

自动化抓取三文鱼相关数据，包括新闻、市场动态、期货价格等。

存储并分析数据，通过AI生成深度报告与视频脚本，用于内容创作与市场引流。



---

2️⃣ 核心流程

Crawlee 抓取 → DuckDB 存储 → AI 分析生成 → 视频脚本输出



---

3️⃣ 数据来源

A. 一线新闻/盘面

iLaks, SalmonBusiness, FishFarmingExpert, IntraFish, TheFishSite, Kyst.no


B. 指数/交易与期货

LFEX, FishPool, Euronext_Salmon


C. 权威统计/监管与机构

SSB_Salmon_Weekly, Mattilsynet, Fiskeridir, NSC_Market, EUMOFA


D. 运输通道/航司与机场

SAS_Cargo, Qatar_Airways_Cargo, Emirates_SkyCargo, PACTL_PVG, SH_Airport_EN


E. 中国监管/到岸合规

GACC_EN_Food


F. 上市公司公告

Mowi, SalMar, Leroy, Grieg, Cermaq, Bakkafrost



---

4️⃣ 数据抓取

工具： Crawlee

内容： 按照 sources/salmon/sources.json 配置，抓取新闻、价格、期货、机构报告、监管数据、运输信息等。

频率： 日更/周更（根据 cadence 配置）。

存储： 抓取的内容存入 DuckDB，表结构包括：

news(ts, source, title, url, lang)

ingest_logs(ts, source, status, detail)




---

5️⃣ 数据分析（AI 生成）

模型： GPT-5 或 Google Gemini

分析内容： 从 DuckDB 提取最新三文鱼信息，通过AI生成分析内容。

输出：

市场动态分析

价格波动解析

供应链与运输通道分析

期货与现货市场对比

法规与政策影响评估




---

6️⃣ 输出

视频脚本： AI生成的分析内容转化为视频脚本用于内容制作。

报告生成： 每日/每周生成市场报告，提供给批发商、进口商、行业分析师等。



---

7️⃣ 白名单规则（AI 分析层执行标准）

输入限制： 只允许 DuckDB 中的合法源进行分析。

输出规则：

AI生成的分析必须有明确数据支持。

禁止主观臆断，保证分析的客观性。




---

8️⃣ 数据清洗与存储

所有抓取的原始数据经过清洗后存入 DuckDB，存储表为：

news: 存储新闻标题、时间戳、来源、摘要等。

ingest_logs: 存储抓取过程中出现的错误与提示，帮助优化数据抓取与修复。




---

9️⃣ 价值交付

日更报告： 三文鱼的日常市场动态和价格变化。

周报： 结合所有抓取的源生成的综合分析报告。

视频脚本： 自动生成的针对三文鱼市场的分析脚本，用于视频生产与观众引流。



---

🔟 系统结构层级图

SalmonAnalysis/
│
├── Production/
│   └── ingest_latest.py         # 主抓取与更新逻辑（调用 Crawlee）
│
├── sources/
│   └── salmon/
│       └── sources.json         # 所有抓取来源配置（分组管理）
│
├── data/
│   └── salmon.duckdb            # 主 DuckDB 数据库，存储所有新闻/日志
│
├── logs/
│   ├── ingest.log               # 抓取执行日志
│   └── error.log                # 异常记录
│
├── ai/
│   ├── summarizer.py            # AI 分析逻辑（基于白名单规则）
│   └── video_script_gen.py      # 生成视频脚本内容
│
└── outputs/
    ├── daily_summary.txt        # 每日摘要
    └── video_script.txt         # 视频脚本输出


---

11️⃣ 日更逻辑

1. 每天运行 Crawlee，自动扫描并提取新内容。


2. 数据进入 DuckDB，去重并打时间戳。


3. AI 自动读取最新内容 → 生成简报、分析语句、视频脚本。


4. 输出用于视频旁白、剪映脚本或 NotebookLM 内容。




---

12️⃣ 项目原则

Crawlee 是核心引擎，功能不替换、不删减。

不删除来源：暂时失败的站点仍保留以便修复。

AI 不猜测：所有结论均基于真实抓取内容。

每日更新：系统默认以 Crawlee 为唯一入口。



---

13️⃣ 白名单规则（AI 分析层执行标准）

输入白名单：

仅允许使用来自 DuckDB 的字段：source, title, url, ts, summary。

不允许调用任何外部 API、新闻聚合器或自媒体内容。

AI 读取内容前，必须验证来源在 sources.json 名单中。


输出白名单：

输出内容仅限：

事实摘要（事实性描述）

逻辑分析（基于数据变化）

视频脚本（带情绪语气但不臆测）


禁止出现预测性词汇（如“必然”、“确定”、“内幕”）。


执行逻辑：

AI 只在 白名单源 + 最新时间戳 的范围内操作。

超出规则的输入自动丢弃，不参与生成。



---

总结：

> “Crawlee 负责事实，DuckDB 负责记忆，AI 负责表达，白名单负责边界。”
这是一个以真实数据驱动、零污染、自动化运行的视频生产系统。
